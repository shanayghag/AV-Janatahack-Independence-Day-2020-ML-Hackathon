{"cells":[{"metadata":{"id":"0SYS47ELtYsE"},"cell_type":"markdown","source":"## Installing & importing necsessary libs","execution_count":null},{"metadata":{"id":"H-qPutxltL-j","outputId":"a75348ec-9a81-478a-8791-98281c804b43","trusted":true},"cell_type":"code","source":"!pip install -q transformers","execution_count":null,"outputs":[]},{"metadata":{"id":"QBO_GKkT-osW","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import LongformerModel, LongformerTokenizer\nfrom tqdm.notebook import tqdm\nfrom transformers import get_linear_schedule_with_warmup","execution_count":null,"outputs":[]},{"metadata":{"id":"aGTtRSki-oo-","outputId":"1e06e8bc-8010-422b-a118-b5434a8e8899","trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","execution_count":null,"outputs":[]},{"metadata":{"id":"__PTOMoD-xDu"},"cell_type":"markdown","source":"## Data Preprocessing","execution_count":null},{"metadata":{"id":"EIMtqAYk-ol6","outputId":"5db7b222-b3d4-4950-ced9-14310f45aba2","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/avjantahack/data/train.csv\")\ndf['list'] = df[df.columns[3:]].values.tolist()\nnew_df = df[['ABSTRACT', 'list']].copy()\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"T505A_R0_bhu"},"cell_type":"markdown","source":"## Model configurations","execution_count":null},{"metadata":{"id":"5s0ou47b-oi7","outputId":"68d67183-066e-458d-d4f5-1989133145e0","trusted":true},"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\nMAX_LEN = 1024\nTRAIN_BATCH_SIZE = 4\nVALID_BATCH_SIZE = 4\nEPOCHS = 3\nLEARNING_RATE = 3e-05\ntokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')","execution_count":null,"outputs":[]},{"metadata":{"id":"Y7kse5hu_ftL"},"cell_type":"markdown","source":"## Custom Dataset Class","execution_count":null},{"metadata":{"id":"mns8GE2b-of5","trusted":true},"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.abstract = dataframe.ABSTRACT\n        self.targets = self.data.list\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.abstract)\n\n    def __getitem__(self, index):\n        abstract = str(self.abstract[index])\n        abstract = \" \".join(abstract.split())\n\n        inputs = self.tokenizer.encode_plus(\n            abstract,\n            None,\n            add_special_tokens = True,\n            max_length = self.max_len,\n            pad_to_max_length = True,\n            truncation = True\n        )\n\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n\n        return{\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'targets': torch.tensor(self.targets[index], dtype=torch.float)\n            }","execution_count":null,"outputs":[]},{"metadata":{"id":"LYMOgME_-7vk","outputId":"2f359e30-9f9a-4b9e-e8df-3eb4b3e2a40c","trusted":true},"cell_type":"code","source":"train_size = 0.8\ntrain_dataset=new_df.sample(frac=train_size,random_state=200)\ntest_dataset=new_df.drop(train_dataset.index).reset_index(drop=True)\ntrain_dataset = train_dataset.reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(new_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","execution_count":null,"outputs":[]},{"metadata":{"id":"Ytxm23L1-_tN","trusted":true},"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","execution_count":null,"outputs":[]},{"metadata":{"id":"9ma2ThrU_ksW"},"cell_type":"markdown","source":"## Longformer model","execution_count":null},{"metadata":{"id":"5pNMKmRo_CHY","outputId":"71ae7457-2f62-474a-9572-05344327bee2","trusted":true},"cell_type":"code","source":"# Creating the customized model, by adding a drop out and a dense layer on top of roberta to get the final output for the model. \n\nclass LongformerClass(torch.nn.Module):\n    def __init__(self):\n        super(LongformerClass, self).__init__()\n        self.longformer = LongformerModel.from_pretrained('allenai/longformer-base-4096')\n        self.drop = torch.nn.Dropout(0.3)\n        self.linear = torch.nn.Linear(768, 6)\n    \n    def forward(self, ids, mask):\n        _, output= self.longformer(ids, attention_mask = mask)\n        output = self.drop(output)\n        output = self.linear(output)\n\n        return output\n\nmodel = LongformerClass()\nmodel.to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"cvotQlC5_qyN"},"cell_type":"markdown","source":"## Hyperparameters & Loss function","execution_count":null},{"metadata":{"id":"WUS_GR0h_GFq","trusted":true},"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)","execution_count":null,"outputs":[]},{"metadata":{"id":"gJb3DoE9_KE1","trusted":true},"cell_type":"code","source":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {\n        \"params\": [\n            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n        ],\n        \"weight_decay\": 0.001,\n    },\n    {\n        \"params\": [\n            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n        ],\n        \"weight_decay\": 0.0,\n    },\n]\n\noptimizer = torch.optim.AdamW(optimizer_parameters, lr=3e-5)\nnum_training_steps = int(len(train_dataset) / TRAIN_BATCH_SIZE * EPOCHS)\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = num_training_steps\n)","execution_count":null,"outputs":[]},{"metadata":{"id":"7XesnZGu_xRM"},"cell_type":"markdown","source":"## Train & Eval Functions\n\n","execution_count":null},{"metadata":{"id":"gdA9FfZS_MXw","trusted":true},"cell_type":"code","source":"def train(epoch):\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0), total=len(training_loader)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%1000==0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\ndef validation(epoch):\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0), total=len(testing_loader)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","execution_count":null,"outputs":[]},{"metadata":{"id":"j7p90999_6CF"},"cell_type":"markdown","source":"## Training Model","execution_count":null},{"metadata":{"id":"umh2WOlC_QYv","outputId":"2944ddaf-b268-43f4-e8ce-793ac5a043ee","trusted":true},"cell_type":"code","source":"EPOCHS = 2\nMODEL_PATH = \"/kaggle/working/model.bin\"\nbest_micro = 0\nfor epoch in range(EPOCHS):\n    train(epoch)\n    outputs, targets = validation(epoch)\n    outputs = np.array(outputs) >= 0.5\n    accuracy = metrics.accuracy_score(targets, outputs)\n    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n    print(f\"Accuracy Score = {accuracy}\")\n    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n    if f1_score_micro > best_micro:\n        torch.save(model.state_dict(), MODEL_PATH)\n        best_micro = f1_score_micro","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(id, abstract):\n    MAX_LENGTH = 512\n    inputs = tokenizer.encode_plus(\n        abstract,\n        None,\n        add_special_tokens=True,\n        max_length=512,\n        pad_to_max_length=True,\n        truncation = True\n    )\n    \n    ids = inputs['input_ids']\n    mask = inputs['attention_mask']\n\n    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n\n    ids = ids.to(device)\n    mask = mask.to(device)\n\n    with torch.no_grad():\n        outputs = model(ids, mask)\n\n    outputs = torch.sigmoid(outputs).squeeze()\n    outputs = np.round(outputs.cpu().numpy())\n    \n    out = np.insert(outputs, 0, id)\n    return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def submit():\n    test_df = pd.read_csv('../input/avjantahack/data/test.csv')\n    sample_submission = pd.read_csv('../input/avjantahack/data/sample_submission_UVKGLZE.csv')\n\n    y = []\n    for id, abstract in tqdm(zip(test_df['ID'], test_df['ABSTRACT']),\n                        total=len(test_df)):\n        out = predict(id, abstract)\n        y.append(out)\n    y = np.array(y)\n    submission = pd.DataFrame(y, columns=sample_submission.columns).astype(int)\n    return submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = submit()\nsubmission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.to_csv(\"/kaggle/working/submission_longformer_base_line.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}